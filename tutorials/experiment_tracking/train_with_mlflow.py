import tempfile
from argparse import ArgumentParser
from typing import Any
from datetime import datetime

import mlflow
import pandas as pd
from joblib import dump
from azureml.core import Workspace
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV

parser = ArgumentParser()
parser.add_argument("--train_dataset", type=str, default="data/train.csv")
parser.add_argument("--test_dataset", type=str, default="data/test.csv")
parser.add_argument("--n_cross_vals", type=int, default=5)
parser.add_argument("--max_depth", default=[2, 5, 10, None])
parser.add_argument("--n_estimators", default=[10, 25, 100])
parser.add_argument("--criterion", default=["gini", "entropy"])

args = parser.parse_args()
# This can be used to log all metrics and artifacts that are generated by the model.
# mlflow.autolog(log_models=True)
workspace = Workspace.from_config()

df_train = pd.read_csv(args.train_dataset)
df_test = pd.read_csv(args.test_dataset)

# We define the hyperparameters we want to tune
param_grid = {
    "n_estimators": args.n_estimators,
    "criterion": args.criterion,
    "max_depth": args.max_depth,
}
# We log the selected hyper-parameters to azureml using mlflow
# You can find the best hyper-parameters in the azureml UI under parameters.

mlflow.set_tracking_uri(workspace.get_mlflow_tracking_uri())

with mlflow.start_run(
    run_name=f"Moon_training-{datetime.now().strftime('%m/%d/%Y,%H:%M:%S')}"
) as run:
    run_id = run.info.run_id
    for param, value in param_grid.items():
        mlflow.log_param(f"gridsearch/{param}", str(value))

    model = RandomForestClassifier()
    grid_search = GridSearchCV(model, param_grid, cv=args.n_cross_vals, n_jobs=-1)

    # We train the model
    grid_search.fit(df_train[["x1", "x2"]], df_train["y"])
    model = grid_search.best_estimator_

    # Here we evaluate the model
    predictions = model.predict(df_test[["x1", "x2"]])
    test_accuracy = accuracy_score(df_test["y"], predictions)

    # We log the accuracy to azureml using mlflow
    # You can see the logged metrics in the azureml UI under the "Metrics" tab
    mlflow.log_metric("test_accuracy", test_accuracy)

    # We log the selected hyper-parameters to azureml using mlflow
    # You can find the best hyper-parameters in the azureml UI under parameters.
    for k, v in grid_search.best_params_.items():
        mlflow.log_param(f"selected/{k}", v)

    # Export the model and log it to azureml using mlflow
    with tempfile.TemporaryDirectory() as tmp_dir:
        dump(model, f"{tmp_dir}/model.joblib")
        mlflow.log_artifact(f"{tmp_dir}/model.joblib")
